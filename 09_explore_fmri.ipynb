{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join as oj\n",
    "import sys, time\n",
    "sys.path.insert(1, oj(sys.path[0], '..'))  # insert parent path\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pmlb\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import data\n",
    "import numpy.linalg as npl \n",
    "import numpy.random as npr\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import ortho_group\n",
    "import pickle\n",
    "import prac_mdl_comp\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.linear_model import RidgeCV, LinearRegression\n",
    "from scipy.io import loadmat\n",
    "import h5py\n",
    "save_dir = 'results'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load the fmri data (takes a little bit of time)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7200, 6555) (540, 6555)\n"
     ]
    }
   ],
   "source": [
    "def load_h5(fname):\n",
    "    f = h5py.File(fname, 'r')\n",
    "    data = np.array(f['data'])\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "out_dir = '/scratch/users/vision/data/gallant/vim_2_crcns'\n",
    "\n",
    "# for fmri data\n",
    "X_train_full = np.array(loadmat(oj(out_dir, 'mot_energy_feats_st.mat'))['S_fin'])\n",
    "X_test_full = np.array(loadmat(oj(out_dir, 'mot_energy_feats_sv.mat'))['S_fin'])\n",
    "\n",
    "'''\n",
    "resps_name = oj(out_dir, 'VoxelResponses_subject1.mat')\n",
    "Y_train = np.array(tables.open_file(resps_name).get_node(f'/rt')[:]) # training responses: 73728 (voxels) x 7200 (timepoints)\n",
    "Y_test = np.array(tables.open_file(resps_name).get_node(f'/rv')[:]) \n",
    "'''\n",
    "\n",
    "# response\n",
    "# Y_train = load_h5(oj(out_dir, 'rt_norm.h5')) # training responses: 73728 (voxels) x 7200 (timepoints)    \n",
    "# Y_test = load_h5(oj(out_dir, 'rv_norm.h5') )\n",
    "# sigmas = load_h5(oj(out_dir, f'out_rva_sigmas.h5'))\n",
    "# (U, alphas, _) = pkl.load(open(oj(out_dir, f'decomp_mot_energy.pkl'), 'rb'))\n",
    "print(X_train_full.shape, X_test_full.shape)\n",
    "\n",
    "# for mnist data\n",
    "# dset = 'mnist'\n",
    "# X, _ = pmlb.fetch_data(dset, return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('ridge_identity_bias_variance.txt', 'rb')\n",
    "# results_identity = pickle.load(file)\n",
    "# file.close()\n",
    "\n",
    "# file = open('ridge_singular_bias_variance.txt', 'rb')\n",
    "# results_singular = pickle.load(file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "label_size = 20\n",
    "mpl.rcParams['xtick.labelsize'] = label_size \n",
    "mpl.rcParams['ytick.labelsize'] = label_size \n",
    "mpl.rcParams['axes.labelsize'] = label_size\n",
    "mpl.rcParams['axes.titlesize'] = label_size\n",
    "mpl.rcParams['figure.titlesize'] = label_size\n",
    "# mpl.rcParams['lines.markersize'] = 8\n",
    "mpl.rcParams['grid.linewidth'] = 2.5\n",
    "mpl.rcParams['legend.fontsize'] = 20\n",
    "# pylab.rcParams['xtick.major.pad']=5\n",
    "# pylab.rcParams['ytick.major.pad']=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run to get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npr.seed(0)\n",
    "num_trials = 20\n",
    "gammas = np.array([5e-2, 1e-1, 0.2, 0.5, 0.75, 0.9, 0.95, 1.,  1.05,  1.25, 1.5,  1.75, 1.95, 2., 2.05, 2.25, 2.5, 5., 10.])\n",
    "\n",
    "noise_std = .1\n",
    "# n_test = 5000\n",
    "n_test = 540 # only have this many poitns\n",
    "\n",
    "# ds = [400]\n",
    "# ns = np.asarray(ds[0]/gammas, dtype=int)\n",
    "\n",
    "ns = np.array([30])\n",
    "ds = np.asarray(ns[0]*gammas, dtype=int)\n",
    "\n",
    "ols_train = np.zeros((num_trials, len(ds), len(ns)))\n",
    "ols_test = np.zeros((num_trials, len(ds), len(ns)))\n",
    "ols_pred_bias = np.zeros((len(ds), len(ns)))\n",
    "ols_pred_variance = np.zeros((len(ds), len(ns)))\n",
    "prac_mdl_comp_arr = np.zeros((num_trials, len(ds), len(ns)))\n",
    "\n",
    "mse_train = np.zeros((num_trials, len(ds), len(ns)))\n",
    "mse_test = np.zeros((num_trials, len(ds), len(ns)))\n",
    "t_pred_bias = np.zeros((len(ds), len(ns)))\n",
    "t_pred_variance = np.zeros((len(ds), len(ns)))\n",
    "\n",
    "\n",
    "## choice of singular or not, just change this prefix\n",
    "prefix = 'ridge_identity_sigma_%d_'%noise_std\n",
    "# prefix = 'ridge_singular_sigma_%d_'%noise_std ##\n",
    "\n",
    "# X_train = npr.randn(max(ns), max(ds))\n",
    "# X_test = npr.randn(n_test, max(ds))\n",
    "X_train = X_train_full[:max(ns), :max(ds)]\n",
    "X_test = X_test_full[:n_test, :max(ds)]\n",
    "\n",
    "d0 = 50\n",
    "\n",
    "beta_fixed = npr.randn(d0)\n",
    "beta_fixed /= npl.norm(beta_fixed)\n",
    "\n",
    "y_train_star = X_train[:, :d0] @ beta_fixed\n",
    "y_test =  X_test[:, :d0] @ beta_fixed\n",
    "\n",
    "for k, n in enumerate(ns):\n",
    "    \n",
    "    for j, d in enumerate(ds):\n",
    "\n",
    "        t_preds = np.zeros((num_trials, n_test))\n",
    "        ols_preds = np.zeros((num_trials, n_test))\n",
    "\n",
    "        for i in range(num_trials):\n",
    "            if i==0:\n",
    "                print(i, d, n)\n",
    "                \n",
    "            y_train = y_train_star + noise_std * npr.randn(n)\n",
    "            \n",
    "            m = RidgeCV(fit_intercept=False, alphas=np.logspace(-4, 4, num=40, base=10))\n",
    "            m.fit(X_train[:, :d], y_train)\n",
    "#             tvars[i, :] = m.coef_\n",
    "            t_preds[i, :] = X_test[:, :d] @ m.coef_\n",
    "            \n",
    "            mse_train[i, j, k] = npl.norm(y_train - X_train[:, :d] @ m.coef_)**2 / n\n",
    "            mse_test[i, j, k] = npl.norm(y_test - t_preds[i, :])**2 /  n_test\n",
    "                        \n",
    "            ols = LinearRegression(fit_intercept=False)\n",
    "            ols.fit(X_train[:, :d], y_train)\n",
    "            ols_preds[i, :] = X_test[:, :d] @ ols.coef_\n",
    "            \n",
    "            ols_train[i, j, k] = npl.norm(y_train - X_train[:, :d] @ ols.coef_)**2 / n\n",
    "            ols_test[i, j, k] = npl.norm(y_test - ols_preds[i, :])**2 / n_test\n",
    "            \n",
    "            # compute prac-mdl\n",
    "            prac_mdl = prac_mdl_comp.prac_mdl_comp(X_train, y_train, variance=noise_std**2)['prac_mdl']\n",
    "            print('prac_mdl', prac_mdl)\n",
    "            prac_mdl_comp_arr[i, j, k] =prac_mdl\n",
    "        \n",
    "        pmean = t_preds.mean(0)\n",
    "        t_pred_variance[j, k] = np.sum((t_preds - pmean)**2)/num_trials/n_test\n",
    "        t_pred_bias[j, k]  = np.sum((pmean-y_test)**2)/n_test\n",
    "\n",
    "        pmean = ols_preds.mean(0)\n",
    "        ols_pred_variance[j, k] = np.sum((ols_preds - pmean)**2)/num_trials/n_test\n",
    "        ols_pred_bias[j, k]  = np.sum((pmean - y_test)**2)/n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 5])\n",
    "for j, n in enumerate(ns):\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.loglog(gammas, prac_mdl_comp_arr.mean(axis=0)[:, j], label='Prac-MDL-COMP', linestyle='--', linewidth=4.)    \n",
    "    plt.loglog(gammas, mse_test.mean(0)[:, j], label='Test Error', linestyle='--', linewidth=4.)\n",
    "    plt.loglog(gammas, t_pred_variance[:, j], label='Variance', linestyle='-', linewidth=1, marker='D', markersize=5, alpha=.8)\n",
    "    plt.loglog(gammas, t_pred_bias[:, j], label='Bias', linestyle='-', linewidth=1, marker='*', markersize=10, alpha=.8)\n",
    "    plt.title('A', fontweight='bold', loc='left', fontsize=20)\n",
    "    plt.title(\"Ridge Regression\", fontsize=20)\n",
    "    plt.xlabel('d/n')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.loglog(gammas, ols_test.mean(0)[:, j], label='Test Error', linestyle='--', linewidth=4.)\n",
    "    plt.loglog(gammas, ols_pred_variance[:, j], label='Variance', linestyle='-', linewidth=1, marker='D', markersize=5, alpha=.8)\n",
    "    plt.loglog(gammas, ols_pred_bias[:, j], label='Bias', linestyle='-', linewidth=1, marker='*', markersize=8, alpha=.8)\n",
    "    plt.axvline(2., color='k', linestyle=':', alpha=0.5)\n",
    "    plt.title('B', fontweight='bold', loc='left', fontsize=20)\n",
    "    plt.title(\"Min. Norm OLS\", fontsize=20)\n",
    "    plt.xlabel('d/n')\n",
    "#     plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.semilogx(gammas, npl.norm(y_test)**2*np.ones_like(gammas)/n_test, label='Test Error', linestyle='--', linewidth=4.)\n",
    "    plt.semilogx(gammas, np.zeros_like(gammas), label='Variance', linestyle='-', linewidth=1, marker='D', markersize=5, alpha=.8)\n",
    "    plt.semilogx(gammas, npl.norm(y_test)**2*np.ones_like(gammas)/n_test, label='Bias', linestyle='-', linewidth=1, marker='*', markersize=10, alpha=.8)\n",
    "    plt.title('C', fontweight='bold', loc='left', fontsize=20)\n",
    "    plt.title(\"The Zero-estimator\",  fontsize=20)\n",
    "    plt.xlabel('d/n')\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(prefix+'bias_variance.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = prefix+'bias_variance.txt'\n",
    "\n",
    "# results = {'ns': ns, 'ds': ds, 'gammas': gammas, 'num_trials': num_trials, 'beta_norm': npl.norm(beta_star),  \n",
    "#            'mse_test': mse_test, 't_pred_variance': t_pred_variance, 't_pred_bias':t_pred_bias,\n",
    "#            'ols_test': ols_test, 'ols_pred_variance': ols_pred_variance, 'ols_pred_bias':ols_pred_bias,\n",
    "           \n",
    "#            'mse': mse_beta, 't_variance': t_variance, 't_bias':t_bias,\n",
    "#            'ols_mse': ols_mse_beta, 'ols_variance': ols_variance, 'ols_bias':ols_bias,\n",
    "#           }\n",
    "# #     'mdl_practice': mdl_practice, 'mdl_theory_lam': mdl_theory_lam, 'mdl_theory':mdl_theory,  'mdl_theory_mean': mdl_theory_mean}\n",
    "# file = open(file_name, 'wb')\n",
    "# pickle.dump(results, file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
